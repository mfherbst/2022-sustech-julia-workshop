{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelisation\n",
    "\n",
    "So far we were only concerned with performing **sequential computation**, i.e. one instruction after the next. But nowadays even the cheapest personal computers feature multiple processor cores (in some cases even multiple physical processors), such that an efficient implementation needs to be able to perform computation in parallel in order to be able to **make use of the power of multiple processor cores** at once.\n",
    "\n",
    "Before we discuss (some) options for parallel programming in Julia, first some general remarks.\n",
    "\n",
    "\n",
    "\n",
    "## Why is parallelisation needed?\n",
    "\n",
    "About 20 years ago it became clear that **physical constraints** on CPU manifacture and operation would make it impossible to increase performance by focusing on single cores. This was the advent of multi-core CPUs.\n",
    "\n",
    "<img src=\"img/42-years-processor-trend.png\" width=700px>\n",
    "\n",
    "(Image from https://www.karlrupp.net/2018/02/42-years-of-microprocessor-trend-data/)\n",
    "\n",
    "\n",
    "## Why is parallelisation hard?\n",
    "\n",
    "Generally writing good parallel code is much harder than writing good sequential code. One indicator why this is the case is **Amdahl's law**:\n",
    "\n",
    "- If the fraction $f$ of my code can be parallelised, the maximal theoretical speedup by employing $n$ cores is given by\n",
    "$$ F(n) = \\frac{1}{1 - f + \\frac{f}{n}} $$\n",
    "or graphically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "F(f, n) = 1 / (1 - f + f / n)\n",
    "\n",
    "p = plot(; xlabel=\"Number of cores\", ylabel=\"Parallel speedup\", legend=:topleft)\n",
    "for f in reverse((0.2, 0.4, 0.6, 0.8, 0.9, 0.95, 0.99))\n",
    "    plot!(p, n -> F(f, n), 1:32, label=\"$(Int(100f))%\", lw=2)\n",
    "end\n",
    "plot!(p, n -> F(1, n), 1:32, label=\"ideal\", ls=:dash, color=:black)\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Note that even with 95% parallelisable code only a maximal speedup of around 9 is feasible.\n",
    "* Generally **doubling** the number of cores leads to **far less than twice** the speedup.\n",
    "* Getting a code to **scale to more than 10 processors** requires **serious work** ... and thus more involved code.\n",
    "* More involved code means that it is more difficult to debug, switch algorithmic ideas etc.\n",
    "\n",
    "**Therefore: At the center of an efficient parallel code is a fast serial code.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of parallelisation\n",
    "\n",
    "We can distinguish multiple forms of parallelism, amongst these\n",
    "\n",
    "* **Instruction level parallelism** (e.g. SIMD)\n",
    "* **Multi-threading**  (shared memory, same process)\n",
    "* **Multi-processing** (shared memory, different process)\n",
    "* **Distributed parallelism** (Distributed memory and processes (e.g. across nodes)\n",
    "\n",
    "Keeping the order, some ways to do [parallel computing](https://docs.julialang.org/en/v1/manual/parallel-computing/) in Julia:\n",
    "\n",
    "* `@simd`, [SIMD.jl](https://github.com/eschnett/SIMD.jl), [LoopVectorization.jl](https://github.com/JuliaSIMD/LoopVectorization.jl)\n",
    "* `Threads.@threads`, `Threads.@spawn`, [FLoops.jl](https://github.com/JuliaFolds/FLoops.jl), [ThreadsX.jl](https://github.com/tkf/ThreadsX.jl) ...\n",
    "* `@spawnat`, `@fetch`, `RemoteChannel`, `SharedArray`\n",
    "* `@spawnat`, `@fetch`, `RemoteChannel`, [DistributedArrays.jl](https://github.com/JuliaParallel/DistributedArrays.jl), [MPI.jl](https://github.com/JuliaParallel/MPI.jl)\n",
    "\n",
    "**In this course:**\n",
    "  * We already explored instruction-level parallelism as part of [16_Performance_Engineering.ipynb](16_Performance_Engineering.ipynb)\n",
    "  * We will discuss threaded parallelism in [21_Multithreading_Basics.ipynb](21_Multithreading_Basics.ipynb) and MPI-based distributed parallelism in [22_MPI_Distributed_Parallelism.ipynb](22_MPI_Distributed_Parallelism.ipynb)."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.8.1",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
